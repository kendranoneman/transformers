{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4e17bc",
   "metadata": {},
   "source": [
    "# ML Final Report\n",
    "### Ben Richardson and Kendra Noneman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526edc3",
   "metadata": {},
   "source": [
    "#### Link for documentation: https://huggingface.co/docs/transformers/model_doc/clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a2fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import matplotlib.pyplot as plt\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ace0dd",
   "metadata": {},
   "source": [
    "### Acoustic Model:\n",
    "\n",
    "##### Hubert: \n",
    "\"Forces the model to learn a combined acoustic and language model over the continuous inputs\":\n",
    "https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/hubert#overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a3139",
   "metadata": {},
   "source": [
    "### Multimodal Model:\n",
    "\n",
    "##### CLIPModel:\n",
    "Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
    " \n",
    "##### from_pretrained:\n",
    "Changes the weights (including mapping hidden states to vocabulary, pruning and initializing weights, tying the weights between the input embeddings and the output embeddings, etc...)\n",
    "https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained\n",
    "\n",
    "Options for CLIP pre-trained openai models (4 of them):\n",
    "https://huggingface.co/openai\n",
    "\n",
    "##### CLIPProcessor:\n",
    "Constructs a CLIP processor which wraps a CLIP feature extractor and a CLIP tokenizer into a single processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad53763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95caea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a65692",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[49406,   320,  1125,   539,   320,  2368, 49407],\n",
      "        [49406,   320,  1125,   539,   320,  1929, 49407]])\n",
      "torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "print(inputs[\"input_ids\"])\n",
    "print(inputs[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d139ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gtts.tts.gTTS object at 0x000002389CDF01C0>\n"
     ]
    }
   ],
   "source": [
    "## Text to Speech, determine pitch contour\n",
    "text_to_convert = \"a photo of a cat\"\n",
    "language = \"en\"\n",
    "speech = gTTS(text=text_to_convert, lang=language, slow=False)\n",
    "print(speech)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ae44af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8333333333333334, 0.6666666666666667, 0.5, 0.33333333333333337, 0.16666666666666674, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "    Error 263 for command:\n",
      "        open speech.mp3\n",
      "    The specified device is not open or is not recognized by MCI.\n",
      "\n",
      "    Error 263 for command:\n",
      "        close speech.mp3\n",
      "    The specified device is not open or is not recognized by MCI.\n",
      "Failed to close the file: speech.mp3\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "\n    Error 263 for command:\n        open speech.mp3\n    The specified device is not open or is not recognized by MCI.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ddfd9bdb56e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mspeech\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplaysound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\playsound.py\u001b[0m in \u001b[0;36m_playsoundWin\u001b[1;34m(sound, block)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Starting'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mwinCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'open {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mwinCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'play {}{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' wait'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Returning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\playsound.py\u001b[0m in \u001b[0;36mwinCommand\u001b[1;34m(*command)\u001b[0m\n\u001b[0;32m     62\u001b[0m                                 '\\n    ' + errorBuffer.raw.decode('utf-16').rstrip('\\0'))\n\u001b[0;32m     63\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexceptionMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mPlaysoundException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexceptionMessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPlaysoundException\u001b[0m: \n    Error 263 for command:\n        open speech.mp3\n    The specified device is not open or is not recognized by MCI."
     ]
    }
   ],
   "source": [
    "# Find the n words with largest auditory cue, and then set the attention mask to 1 only for those words\n",
    "\n",
    "audio_file = \"speech.mp3\"\n",
    "inputs[\"attention_mask\"] = torch.Tensor([[1,1,1,1,1,1,1],[1,1,1,1,1,1,1]])\n",
    "#torch.Tensor([np.linspace(1,0,num=inputs[\"input_ids\"].shape[1]).tolist(),np.linspace(1,0,num=inputs[\"input_ids\"].shape[1]).tolist()])\n",
    "print(np.linspace(1,0,num=inputs[\"input_ids\"].shape[1]).tolist())\n",
    "\n",
    "speech.save(audio_file)\n",
    "playsound(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa75601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa0187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7555a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c305ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
